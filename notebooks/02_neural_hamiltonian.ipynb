{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e233a1-91e6-492f-9698-84559dcb76e1",
   "metadata": {},
   "source": [
    "# 02 â€¢ Neural Hamiltonian\n",
    "\n",
    "Consider again the Hamiltonian of harmonic oscillator in one dimension:\n",
    "\n",
    "$$H(q, p) = T(p) + V(q) = \\frac{p^2}{2m} + \\frac{k}{2} (q-q_0)^2$$\n",
    "\n",
    "- $T(p)$ and $V(q)$ are the kinetic and potential energies,\n",
    "- $q$ and $p$ are the generalized coordinate and momentum,\n",
    "- $m$ is the mass, $k$ the spring constant, and $q_0$ the equilibrium position.\n",
    "\n",
    "In notebook 01, we have used the Hamiltonian `HarmonicOscillator` to evaluate and simulate the above system. We are now using a `NeuralHamiltonian` to learn the above system from observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981d679-fb0d-475b-a804-1dedd42712f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from hamiltonian import HarmonicOscillator, NeuralHamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df91ac0-be73-4040-ab6d-09b55342eb21",
   "metadata": {},
   "source": [
    "## Ground truth\n",
    "\n",
    "The `HarmonicOscillator` acts as our ground truth, $H_{\\text{exact}}$, for generating training data and evaluation of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95ae5b-8c7b-45f3-924c-5a18ce2548ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_exact = HarmonicOscillator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8469418-72b3-4a54-a5fd-be02356065c3",
   "metadata": {},
   "source": [
    "## Neural Hamiltonian\n",
    "\n",
    "The `NeuralHamiltonian` inherits from our base `Hamiltonian` class, but instead of implementing kinetic and potential energy ($T$, $V$) analytically, it uses a neural network to model their sum ($H=T+V$). It is a generic Hamiltonian that can compute the equations of motion in canonical form, but it needs to be trained on external data to adapt to a specific behavior.\n",
    "\n",
    "The training can be done in multiple ways. Here we are looking at two options, gradient matching and trajectory matching.\n",
    "\n",
    "## Gradient matching\n",
    "\n",
    "We train our neural Hamiltonian on the gradients\n",
    "\n",
    "$$\\frac{\\text{d}q}{\\text{d}t}(q, p) = \\frac{\\partial H}{\\partial p}(q, p) \\quad \\text{and} \\quad \\frac{\\text{d}p}{\\text{d}t}(q, p) = -\\frac{\\partial H}{\\partial q}(q, p).$$\n",
    "\n",
    "In detail, we are sampling random generalized coordinates and momenta\n",
    "\n",
    "$$(q_n, p_n), \\quad n=1,\\dots,N$$\n",
    "\n",
    "and compute the true time derivatives according to the exact Hamiltonian $H_{\\text{exact}}$:\n",
    "\n",
    "$$\\frac{\\text{d}q_{\\text{true}}}{\\text{d}t}(q_n, p_n) = \\frac{\\partial H_{\\text{exact}}}{\\partial p}(q_n, p_n) \\quad \\text{and} \\quad \\frac{\\text{d}p_{\\text{true}}}{\\text{d}t}(q_n, p_n) = -\\frac{\\partial H_{\\text{exact}}}{\\partial q}(q_n, p_n).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e52966c-83a6-432c-982e-abda22f80ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(batch, amplitude=1.0):\n",
    "    q = torch.rand(batch, 1).sub(0.5).mul(2 * amplitude)\n",
    "    p = torch.rand(batch, 1).sub(0.5).mul(2 * amplitude)\n",
    "    return q, p\n",
    "\n",
    "\n",
    "q_sample, p_sample = sample(5)\n",
    "H_exact.time_derivatives(q_sample, p_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedab7ee-6ca5-4d68-a8a2-856441bdec98",
   "metadata": {},
   "source": [
    "We then create a neural Hamiltonian, $H_{\\text{neural}}$, and compare its predicted time derivatives\n",
    "\n",
    "$$\\frac{\\text{d}q_{\\text{pred}}}{\\text{d}t}(q_n, p_n) = \\frac{\\partial H_{\\text{neural}}}{\\partial p}(q_n, p_n) \\quad \\text{and} \\quad \\frac{\\text{d}p_{\\text{pred}}}{\\text{d}t}(q_n, p_n) = -\\frac{\\partial H_{\\text{neural}}}{\\partial q}(q_n, p_n)$$\n",
    "\n",
    "against the true time derivatives with a mean squared error loss:\n",
    "\n",
    "$$\\mathcal{L} = N^{-1} \\sum\\limits_{n=1}^{N} \\left( \\left( \\frac{\\text{d}q_{\\text{true}}}{\\text{d}t}(q_n, p_n) - \\frac{\\text{d}q_{\\text{pred}}}{\\text{d}t}(q_n, p_n) \\right)^2 + \\left( \\frac{\\text{d}p_{\\text{true}}}{\\text{d}t}(q_n, p_n) - \\frac{\\text{d}p_{\\text{pred}}}{\\text{d}t}(q_n, p_n) \\right)^2  \\right).$$\n",
    "\n",
    "Thus, the neural Hamiltonian learns to replicate the time deriviaties of the exact Hamiltonian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb818a1a-068f-464d-ac48-4f009fbab4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_neural = NeuralHamiltonian(shape=(1,), hidden=16)\n",
    "optimizer = torch.optim.Adam(H_neural.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, 10001):\n",
    "    optimizer.zero_grad()\n",
    "    q, p = sample(50, amplitude=1.5)\n",
    "    dq_dt_true, dp_dt_true = H_exact.time_derivatives(q, p)\n",
    "    dq_dt_pred, dp_dt_pred = H_neural.time_derivatives(q, p)\n",
    "    loss = (\n",
    "        (dq_dt_true - dq_dt_pred).pow(2) + (dp_dt_true - dp_dt_pred).pow(2)\n",
    "    ).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch == 1 or epoch % 500 == 0:\n",
    "        print(f\"epoch: {epoch:5d} | loss: {loss.item():.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb2fc7-22e4-481c-b876-a56908acf811",
   "metadata": {},
   "source": [
    "To evaluate our trained model, we compare simulated trajectories (2000 steps) and measure deviations in the propagated generalized coordinates and momenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9d641-985f-48db-b300-9fc827ff3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(H, q, p, delta_t, steps):\n",
    "    qt, pt = [q.unsqueeze(0)], [p.unsqueeze(0)]\n",
    "    for _ in range(steps):\n",
    "        q, p = H.step(q, p, delta_t)\n",
    "        qt.append(q.unsqueeze(0))\n",
    "        pt.append(p.unsqueeze(0))\n",
    "    return torch.cat(qt, dim=0).detach(), torch.cat(pt, dim=0).detach()\n",
    "\n",
    "\n",
    "q = torch.tensor([1.0])  # initial coordinate\n",
    "p = torch.tensor([0.0])  # initial momentum\n",
    "\n",
    "steps = 2000  # make 2000 steps\n",
    "delta_t = 0.01  # time step\n",
    "\n",
    "qt_true, pt_true = simulate(H_exact, q, p, delta_t, steps)\n",
    "qt_pred, pt_pred = simulate(H_neural, q, p, delta_t, steps)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(8, 3))\n",
    "\n",
    "axes[0].plot(qt_true, color=\"C0\", label=\"q (true)\")\n",
    "axes[0].plot(qt_pred, \"--\", color=\"C0\", label=\"q (pred)\")\n",
    "axes[0].plot(pt_true, color=\"C1\", label=\"p (true)\")\n",
    "axes[0].plot(pt_pred, \"--\", color=\"C1\", label=\"p (pred)\")\n",
    "axes[0].set_ylabel(\"q, p / a.u.\")\n",
    "\n",
    "axes[1].plot(qt_true - qt_pred, label=\"q\")\n",
    "axes[1].plot(pt_true - pt_pred, label=\"p\")\n",
    "axes[1].set_ylabel(\"error / a.u.\")\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel(\"step\")\n",
    "    ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a438a-d83f-43dc-b50c-52f17435d249",
   "metadata": {},
   "source": [
    "Both true and predicted trajectories $(q(t), p(t))$ stay very close. The error oscilates around zero with an amplitude that increases over time.\n",
    "\n",
    "## Trajectory matching\n",
    "\n",
    "In another approach, train the neural Hamiltonian on observed trajectory data, i.e., trajectories $(q_{\\text{true}}(t_n), p_{\\text{true}}(t_n))$, $n=0,\\dots,N$, generated by the exact Hamiltonian.\n",
    "\n",
    "The aim is that the neural Hamiltonian learns to correctly propagate\n",
    "\n",
    "$$H_{\\textrm{neural}}:(q_{\\text{true}}(t_{n-1}), p_{\\text{true}}(t_{n-1})) \\mapsto (q_{\\text{pred}}(t_n), p_{\\text{pred}}(t_n))$$\n",
    "\n",
    "such that the mean squared error on the generalized coordinates and momenta becomes minimal:\n",
    "\n",
    "$$\\mathcal{L} = N^{-1} \\sum\\limits_{n=1}^{N} \\left( \\left( q_{\\text{true}}(t_{n}) - q_{\\text{pred}}(t_{n}) \\right)^2 + \\left( p_{\\text{true}}(t_{n}) - p_{\\text{pred}}(t_{n}) \\right)^2  \\right).$$\n",
    "\n",
    "In this particular example, we\n",
    "\n",
    "- sample 20 random initial coordinates and momenta\n",
    "- propagate those simultaneously over 200 steps with $H_{\\text{exact}}$\n",
    "- subsample by keeping only every 50th step ($t_0, t_{50}, t_{100}, t_{150}, t_{200}$) which gives us four propagations per initial condition\n",
    "- replicate the 50 step propagations with $H_{\\text{neural}}$\n",
    "- minimize the mean squared error between the true and predicted propagated coordinates and momenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622b1b6-225c-4e9f-a000-96efc8ab5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ini, p_ini = sample(20, amplitude=1.5)\n",
    "subsample = 50\n",
    "delta_t = 0.01\n",
    "\n",
    "qt, pt = simulate(H_exact, q_ini.squeeze(), p_ini.squeeze(), delta_t, 200)\n",
    "qt = qt[::subsample].T\n",
    "pt = pt[::subsample].T\n",
    "\n",
    "q0, q_true = qt[:, :-1].reshape(-1, 1), qt[:, 1:].reshape(-1, 1)\n",
    "p0, p_true = pt[:, :-1].reshape(-1, 1), pt[:, 1:].reshape(-1, 1)\n",
    "\n",
    "H_neural = NeuralHamiltonian(shape=(1,), hidden=16)\n",
    "optimizer = torch.optim.Adam(H_neural.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, 2001):\n",
    "    q_pred, p_pred = q0.clone().detach(), p0.clone().detach()\n",
    "    optimizer.zero_grad()\n",
    "    for _ in range(subsample):\n",
    "        q_pred, p_pred = H_neural.step(q_pred, p_pred, delta_t)\n",
    "    loss = ((q_true - q_pred).pow(2) + (p_true - p_pred).pow(2)).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch == 1 or epoch % 100 == 0:\n",
    "        print(f\"epoch: {epoch:5d} | loss: {loss.item():.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2c2c3-e7ab-47c8-9ee6-709f51e8f2ce",
   "metadata": {},
   "source": [
    "We again compare simulated trajectories (2000 steps) and measure deviations in the propagated generalized coordinates and momenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a6986-6e2b-4359-a5c8-a8066cddf27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.tensor([1.0])  # initial coordinate\n",
    "p = torch.tensor([0.0])  # initial momentum\n",
    "\n",
    "steps = 2000  # make 2000 steps\n",
    "delta_t = 0.01  # time step\n",
    "\n",
    "qt_true, pt_true = simulate(H_exact, q, p, delta_t, steps)\n",
    "qt_pred, pt_pred = simulate(H_neural, q, p, delta_t, steps)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(8, 3))\n",
    "\n",
    "axes[0].plot(qt_true, color=\"C0\", label=\"q (true)\")\n",
    "axes[0].plot(qt_pred, \"--\", color=\"C0\", label=\"q (pred)\")\n",
    "axes[0].plot(pt_true, color=\"C1\", label=\"p (true)\")\n",
    "axes[0].plot(pt_pred, \"--\", color=\"C1\", label=\"p (pred)\")\n",
    "axes[0].set_ylabel(\"q, p / a.u.\")\n",
    "\n",
    "axes[1].plot(qt_true - qt_pred, label=\"q\")\n",
    "axes[1].plot(pt_true - pt_pred, label=\"p\")\n",
    "axes[1].set_ylabel(\"error / a.u.\")\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel(\"step\")\n",
    "    ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100eaab-2eb4-4beb-89cf-2a43404fced0",
   "metadata": {},
   "source": [
    "We can observe the same behavior again:\n",
    "- both true and predicted trajectories $(q(t), p(t))$ stay very close\n",
    "- the error oscilates around zero with an amplitude that increases over time\n",
    "\n",
    "To summarize, a neural Hamiltonian can learn the dynamics of an exact Hamiltonian via gradient or trajectory matching.\n",
    "\n",
    "In the next notebook, we will look at a double well potential and thermostatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a80700-a6c7-4f03-bfa6-04868db8edd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
